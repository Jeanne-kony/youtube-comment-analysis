{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeanne-kony/youtube-comment-analysis/blob/main/youtube_comments_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UJ692-sBq-2E"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install streamlit\n",
        "!pip install google-api-python-client\n",
        "!pip install matplotlib\n",
        "!pip install wordcloud\n",
        "!pip install ipadic\n",
        "!pip install beautifulsoup4\n",
        "!pip install mecab-python3\n",
        "!pip install SudachiPy==0.5.4 ginza==4.0.6 ja-ginza==4.0.0\n",
        "!pip install japanize-matplotlib\n",
        "!pip install pyvis\n",
        "!pip install streamlit openai\n",
        "!pip install bertopic==0.16.0\n",
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK9t0CTZW-w7"
      },
      "outputs": [],
      "source": [
        "#パッケージ読み込みのエラーを回避\n",
        "import pkg_resources, imp\n",
        "imp.reload(pkg_resources)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcxTtKXcV6ra"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# 必要なライブラリのインポート\n",
        "import streamlit as st\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import MeCab\n",
        "from collections import Counter\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import ipadic\n",
        "import re\n",
        "import os\n",
        "import networkx as nx\n",
        "import matplotlib.cm as cm\n",
        "import japanize_matplotlib\n",
        "from networkx.algorithms.community import greedy_modularity_communities\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import openai\n",
        "from pathlib import Path\n",
        "from bertopic import BERTopic\n",
        "from googleapiclient.discovery import build\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "import yaml\n",
        "from yaml.loader import SafeLoader\n",
        "import streamlit_authenticator as stauth\n",
        "from streamlit_authenticator.utilities.hasher import Hasher\n",
        "\n",
        "# 日本語の自然言語処理モデルの読み込み\n",
        "nlp = spacy.load('ja_ginza')\n",
        "\n",
        "# APIキーなどの定数の定義\n",
        "API_KEY = \"YouTube Data API v3のAPIキーを入力してください\"\n",
        "OPENAI_API_KEY = \"OPENAIのAPIキーを入力してください\"\n",
        "MAX_COMMENTS = 1000\n",
        "\n",
        "# OpenAIとYouTube APIの設定\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
        "\n",
        "# YouTubeのコメントを取得する関数\n",
        "def get_comments(video_id, pageToken):\n",
        "    comment_url = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
        "    param = {\n",
        "        \"key\": API_KEY,\n",
        "        \"videoId\": video_id,\n",
        "        \"part\": \"replies,snippet\",\n",
        "        \"maxResults\": \"100\",\n",
        "        \"pageToken\": pageToken if pageToken else None\n",
        "    }\n",
        "    req = requests.get(comment_url, params=param)\n",
        "    return req.json()\n",
        "\n",
        "# YouTube動画URLからビデオIDを抽出する関数\n",
        "def extract_video_id(url):\n",
        "    video_id_match = re.search(r'(?:v=|\\/(?:embed|v|shorts)\\/|youtu\\.be\\/)([^&\\/?]+)', url)\n",
        "    return video_id_match.group(1) if video_id_match else None\n",
        "\n",
        "# HTMLタグを除去する関数\n",
        "def remove_html_tags(text):\n",
        "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "# テキストを形態素解析してトークン化する関数\n",
        "def mecab_tokenizer(text):\n",
        "    tagger = MeCab.Tagger(ipadic.MECAB_ARGS)\n",
        "    replaced_text = preprocess_text(text)\n",
        "    parsed_lines = tagger.parse(replaced_text).split(\"\\n\")[:-2]\n",
        "    surfaces = [l.split(\"\\t\")[0] for l in parsed_lines]\n",
        "    pos = [l.split(\"\\t\")[1].split(\",\")[0] for l in parsed_lines]\n",
        "    target_pos = [\"名詞\", \"動詞\", \"形容詞\"]\n",
        "    token_list = [t for t, p in zip(surfaces, pos) if p in target_pos]\n",
        "    kana_re = re.compile(\"^[ぁ-ゖ]+$\")  #い（一段、連用形）などを削除\n",
        "    return [t for t in token_list if not kana_re.match(t)]\n",
        "\n",
        "# テキストの前処理を行う関数\n",
        "def preprocess_text(text):\n",
        "    text = unicodedata.normalize(\"NFKC\", text).upper()  #全ての文字を大文字化\n",
        "    text = re.sub(r'[【】 () （） 『』　「」]', '', text)\n",
        "    text = re.sub(r'[\\[\\］\\]]', ' ', text)\n",
        "    text = re.sub(r'[@＠]\\w+', '', text)\n",
        "    text = re.sub(r'\\d+\\.*\\d*', '', text)\n",
        "    return text\n",
        "\n",
        "# 特定の品詞の単語を抽出する関数\n",
        "def extract_words(sent, pos_tags):\n",
        "    return [token.lemma_ for token in sent if token.pos_ in pos_tags and token.lemma_]\n",
        "\n",
        "# 単語の共起関係を計算する関数\n",
        "def count_cooccurrence(sents, token_length='{2,}'):\n",
        "    count_model = CountVectorizer(token_pattern=f'\\\\b\\\\w{token_length}\\\\b') #1文字ではない英単語を抽出\n",
        "    X = count_model.fit_transform(sents)\n",
        "    words = count_model.get_feature_names_out()\n",
        "    word_counts = np.asarray(X.sum(axis=0)).reshape(-1)\n",
        "    X[X > 0] = 1 #単語の出現回数の差異を無視するためニ値化\n",
        "    Xc = (X.T * X)\n",
        "    return words, word_counts, Xc, X\n",
        "\n",
        "# 共起ネットワークを作成する関数\n",
        "def create_network(words, word_counts, Xc, weight_cutoff):\n",
        "    G = nx.Graph()\n",
        "    weights_w = [(word, {'weight': count / word_counts.max()}) for word, count in zip(words, word_counts)]\n",
        "    G.add_nodes_from(weights_w)\n",
        "    Xc_max = Xc.max()\n",
        "    weights_c = [(words[i], words[j], Xc[i,j] / Xc_max) for i, j in zip(*Xc.nonzero()) if i < j and Xc[i,j] > weight_cutoff * Xc_max]\n",
        "    G.add_weighted_edges_from(weights_c)\n",
        "    G.remove_nodes_from(list(nx.isolates(G)))\n",
        "    return G\n",
        "\n",
        "# ネットワークを可視化する関数\n",
        "def pyplot_network(G, layout, layout_parameter_k, weight_cutoff, node_size_, text_size):\n",
        "    plt.figure(figsize=(25, 10), dpi=300)\n",
        "    pos = get_layout(G, layout, layout_parameter_k)\n",
        "    connecteds = list(greedy_modularity_communities(G))\n",
        "    colors_array = cm.Pastel1(np.linspace(0.1, 0.9, len(connecteds)))\n",
        "    node_colors = [next(colors_array[i] for i, c in enumerate(connecteds) if node in c) for node in G.nodes()]\n",
        "    weights_n = np.array(list(nx.get_node_attributes(G, 'weight').values()))\n",
        "    weights_e = np.array(list(nx.get_edge_attributes(G, 'weight').values()))\n",
        "    nx.draw_networkx_nodes(G, pos, alpha=0.3, node_color=node_colors, node_size=node_size_ * weights_n)\n",
        "    nx.draw_networkx_edges(G, pos, alpha=0.4, edge_color=\"whitesmoke\", width=20 * weights_e)\n",
        "    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic', font_size=text_size)\n",
        "    plt.axis(\"off\")\n",
        "    st.pyplot(plt)\n",
        "\n",
        "# ネットワークのレイアウトを取得する関数\n",
        "def get_layout(G, layout, layout_parameter_k):\n",
        "    if layout == 'spring':\n",
        "        return nx.spring_layout(G, k=layout_parameter_k, iterations=50, weight='weight')\n",
        "\n",
        "# コメントから批判点を要約する関数\n",
        "def summarize_criticisms(comments):\n",
        "    prompt = f\"以下のコメントから動画に対する批判の内容を総括してください。\\n\\n\" + \"\\n\".join(comments)\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# メイン関数：Streamlitアプリケーションの主要ロジック\n",
        "def main():\n",
        "        st.title('YouTube Comments Analysis')\n",
        "        video_url = st.text_input('Enter YouTube Video URL')\n",
        "\n",
        "        if video_url:\n",
        "            video_id = extract_video_id(video_url)\n",
        "            if video_id:\n",
        "                st.write(f'Extracted Video ID: {video_id}')\n",
        "                comments = fetch_comments(video_id)\n",
        "                if comments:\n",
        "                    tab1, tab2, tab3 = st.tabs([\"word cloud\", \"word co-occurrence networks\",\"clustering\"])\n",
        "                    with tab1:\n",
        "                      display_word_cloud(comments)\n",
        "                    with tab2:\n",
        "                      display_word_cooccurrence_network(comments)\n",
        "                    with tab3:\n",
        "                      display_topic_clustering(comments)\n",
        "                    display_criticism_summary(comments)\n",
        "                else:\n",
        "                    st.write(\"No comments found or error occurred while fetching comments.\")\n",
        "            else:\n",
        "                st.write('Invalid YouTube URL')\n",
        "\n",
        "# コメントを取得する関数\n",
        "def fetch_comments(video_id):\n",
        "    comments = []\n",
        "    pageToken = \"\"\n",
        "    current_comments = 0\n",
        "    with st.spinner('Fetching comments...'):\n",
        "        progress_bar = st.progress(0)\n",
        "        while pageToken is not None and current_comments < MAX_COMMENTS:\n",
        "            req = get_comments(video_id, pageToken)\n",
        "            if 'items' in req:\n",
        "                for comment_thread in req[\"items\"]:\n",
        "                    if current_comments >= MAX_COMMENTS:\n",
        "                        break\n",
        "                    snippet = comment_thread[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
        "                    comments.append(remove_html_tags(snippet[\"textDisplay\"]))\n",
        "                    current_comments += 1\n",
        "                    progress_bar.progress(current_comments / MAX_COMMENTS)\n",
        "                    if \"replies\" in comment_thread:\n",
        "                        for reply in comment_thread[\"replies\"][\"comments\"]:\n",
        "                            if current_comments >= MAX_COMMENTS:\n",
        "                                break\n",
        "                            comments.append(remove_html_tags(reply[\"snippet\"][\"textDisplay\"]))\n",
        "                            current_comments += 1\n",
        "                            progress_bar.progress(current_comments / MAX_COMMENTS)\n",
        "                pageToken = req.get(\"nextPageToken\")\n",
        "            else:\n",
        "                pageToken = None\n",
        "    return comments\n",
        "\n",
        "# ワードクラウドを表示する関数\n",
        "def display_word_cloud(comments):\n",
        "    words = ' '.join(mecab_tokenizer(' '.join(comments)))\n",
        "    font_path = \"フォントのパスを入力してください\"\n",
        "    stopwords_path = \"stopwordsのパスを入力してください\"\n",
        "    with open(stopwords_path, \"r\") as f:\n",
        "        stopwords = f.read().split(\"\\n\")\n",
        "\n",
        "    wordcloud = WordCloud(\n",
        "        background_color=\"white\",\n",
        "        width=800,\n",
        "        height=800,\n",
        "        font_path=font_path,\n",
        "        colormap=\"viridis\",\n",
        "        stopwords=list(set(stopwords)),\n",
        "        max_words=100,\n",
        "    ).generate(words)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    ax.axis(\"off\")\n",
        "    st.pyplot(fig)\n",
        "\n",
        "# 単語の共起ネットワークを表示する関数\n",
        "def display_word_cooccurrence_network(comments):\n",
        "    words = ' '.join(mecab_tokenizer(' '.join(comments)))\n",
        "    doc = preprocess_text(words)\n",
        "    docs = [nlp(words)]\n",
        "\n",
        "    include_pos = ('NOUN', 'PROPN')\n",
        "    sents = [' '.join(extract_words(sent, include_pos)) for words in docs for sent in words.sents]\n",
        "    words, word_counts, Xc, X = count_cooccurrence(sents, token_length='{1,}')\n",
        "\n",
        "    G = create_network(words, word_counts, Xc, 0.025)\n",
        "    pyplot_network(G, 'spring', 0.4, 0.025, 7500, 9)\n",
        "\n",
        "# トピッククラスタリングを表示する関数\n",
        "def display_topic_clustering(comments):\n",
        "    vectorizer = CountVectorizer(tokenizer=mecab_tokenizer)\n",
        "    model = BERTopic(\n",
        "        embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "        vectorizer_model=vectorizer,\n",
        "        language=\"japanese\",\n",
        "        calculate_probabilities=True,\n",
        "        verbose=True,\n",
        "        nr_topics=\"20\"\n",
        "    )\n",
        "    topics, probs = model.fit_transform(comments)\n",
        "\n",
        "    if len(set(topics)) > 1:\n",
        "        fig = model.visualize_barchart()\n",
        "        st.plotly_chart(fig)\n",
        "    else:\n",
        "        st.write(\"コメント数が不足しているため、ビジュアライゼーションができません。\")\n",
        "\n",
        "# 批判点の要約を表示する関数\n",
        "def display_criticism_summary(comments):\n",
        "    summary = summarize_criticisms(comments)\n",
        "    st.subheader(\"批判点の総括:\")\n",
        "    st.write(summary)\n",
        "\n",
        "# メイン関数の実行\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ec8gIp_nhBm"
      },
      "outputs": [],
      "source": [
        "!curl https://loca.lt/mytunnelpassword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiyglWPrxr8M"
      },
      "outputs": [],
      "source": [
        "#streamlitをlocaltunnelのトンネルで起動\n",
        "#実行画面に出るyour url is:のurlに接続し、ブラウザの別タブに移動\n",
        "#Click to Continueでstreamlitの動作を確認\n",
        "#確認が終わったらこのセルの実行を停止する\n",
        "!streamlit run app.py & sleep 3 && npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12_DFzZjfFlN1tul05-SK5tZXg-0TiK4x",
      "authorship_tag": "ABX9TyM3G+eN3IiyYYu368wo8v3d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}